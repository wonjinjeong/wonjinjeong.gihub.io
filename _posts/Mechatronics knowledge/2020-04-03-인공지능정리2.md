---
layout: post
section-type: post
title: 인공지능 정리2
category: tech
tags: ['mechatronics', 'AI', 'hunkim', 'pytorch']
published: true
---

logistic regression을 다시 말해보면 H(x)(결과 값)가 너무 큰것을 쉽게 비교할 수 있는 값으로 범위를 줄이고 싶을 때 sigmoid 방법을 이용하여 표현 했고 이 식을 정리하면 다음과 같다

<img src="/img/mechatronics/R1.png" alt="" width="600" height="130">

W(weight)를 학습한다. -> 구분하는 법을 찾아낸다로 이해할 수 있는데

만약 변수가 binary가 아닐 때 다중일 때는 어떻게 될까?
그 방법은 각각 해당되는 예측값을 학습시켜주면 된다.

<img src="/img/mechatronics/R2.png" alt="" width="600" height="130">

각각의 가중치가 곱해진 결과값은 예측값으로 다음과 같이 정리된다.

<img src="/img/mechatronics/R3.png" alt="" width="600" height="130">

learning rate를 조절하는 알파값을 gredient descent에서 볼 수 있는데 임의로 이 값을 기입해 정했는데 이 값은 매우 중요하다.<br>
만약 learning rate가 너무 크다면 overshooting이 발생할 수 있다.
<img src="/img/mechatronics/overshooting.png" alt="" width="600" height="130">
<br>

반대로 learning rate가 너무 작다면 너무 오래 학습이 걸리게 되고 최저점이 아닌경우에도 학습이 멈출 수 있다.

그래서 cost function을 확인하면서 learning rate를 변화를 주는것이 제일 좋다.

보통 data 값이 차이가 많이 나게 되면 그 데이터를 조정해주는데 기법으로는 zero-centered data, normalized data 하는 방법이 있다.
<img src="/img/mechatronics/standardization.png" alt="" width="600" height="130">
<br>
X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()

머신러닝의 문제점 중 overfitting이란게 있는데 너무 학습 데이터에 너무 잘 맞는 문제를 예로 들 수 있다.
<img src="/img/mechatronics/overfitting.png" alt="" width="600" height="130">
<br>

이를 방지하기 위해 사용하는 방식은<br>
1.More training data<br>
2.Reduce the number of features<br>
3.Regularization

Regularization이란 것은 cost함수의 최적화 식에 w(각각의 element)를 제곱을 한 값이 작아질 수 있도록 하는 것이다.
여기서 제곱앞의 변수는 regularization strength로 부른다.
<img src="/img/mechatronics/regularization_function.png" alt="" width="600" height="130">
<br>