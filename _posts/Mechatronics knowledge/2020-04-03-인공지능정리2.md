---
layout: post
section-type: post
title: 인공지능 정리2
category: tech
tags: ['mechatronics', 'AI', 'hunkim', 'pytorch']
published: true
---

logistic regression을 다시 말해보면 H(x)(결과 값)가 너무 큰것을 쉽게 비교할 수 있는 값으로 범위를 줄이고 싶을 때 sigmoid 방법을 이용하여 표현 했고 이 식을 정리하면 다음과 같다

<img src="/img/mechatronics/R1.png" alt="" width="600" height="130">

W(weight)를 학습한다. -> 구분하는 법을 찾아낸다로 이해할 수 있는데

만약 변수가 binary가 아닐 때 다중일 때는 어떻게 될까?
그 방법은 각각 해당되는 예측값을 학습시켜주면 된다.

<img src="/img/mechatronics/R2.png" alt="" width="600" height="130">

각각의 가중치가 곱해진 결과값은 예측값으로 다음과 같이 정리된다.

<img src="/img/mechatronics/R3.png" alt="" width="600" height="130">

learning rate를 조절하는 알파값을 gredient descent에서 볼 수 있는데 임의로 이 값을 기입해 정했는데 이 값은 매우 중요하다.<br>
만약 learning rate가 너무 크다면 overshooting이 발생할 수 있다.
<img src="/img/mechatronics/overshooting.png" alt="" width="600" height="130">
<br>

반대로 learning rate가 너무 작다면 너무 오래 학습이 걸리게 되고 최저점이 아닌경우에도 학습이 멈출 수 있다.

그래서 cost function을 확인하면서 learning rate를 변화를 주는것이 제일 좋다.

보통 data 값이 차이가 많이 나게 되면 그 데이터를 조정해주는데 기법으로는 zero-centered data, normalized data 하는 방법이 있다.
<img src="/img/mechatronics/standardization.png" alt="" width="600" height="130">
<br>
X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()

머신러닝의 문제점 중 overfitting이란게 있는데 너무 학습 데이터에 너무 잘 맞는 문제를 예로 들 수 있다.
<img src="/img/mechatronics/overfitting.png" alt="" width="600" height="130">
<br>

이를 방지하기 위해 사용하는 방식은<br>
1.More training data<br>
2.Reduce the number of features<br>
3.Regularization

Regularization이란 것은 cost함수의 최적화 식에 w(각각의 element)를 제곱을 한 값이 작아질 수 있도록 하는 것이다.
여기서 제곱앞의 변수는 regularization strength로 부른다.
<img src="/img/mechatronics/regularization_function.png" alt="" width="600" height="130">
<br>

학습한 모델이 얼마나 좋은지 어떻게 알 수 있을까?<br>
모델을 training set으로 학습을 하게 되는데 만약 데이터를 전부(100%)다 학습을 시키면 좋은 방법이 아니다.<br>
대략 일부분을 test set로 숨겨놓고 나머지 부분을 training set으로 학습을 시키면 좋은 모델을 형성할 수 있고 이를 test set으로 예측값을 확인해본다. 예로 training set을 교과서로 생각하고 test set을 시험지라고 생각하면 편하다.
<img src="/img/mechatronics/training_test_set.jpg" alt="" width="600" height="130">
<br>
위 그림에서 validation을 모의시험이라고 생각하면 된다.<br>

또 Online learning이란 방법이 있는데<br>
예를 들어 100만개의 data가 있다고 했을 때 10만개씩 데이터를 학습을 시키게 되면 이후에 데이터가 새로 왔을 때 추가로 데이터를 학습시킬 수 있어 사용하게 된다.<br>

우리가 궁극적으로 인공지능에서 원하는것은 생각하는 기계를 만드는 것이다.<br>
이에 우리가 인간의 뉴런구조에서 모방해 Activation Functions을 구성하였는데 이를 그림으로 보면 다음과 같다.
<img src="/img/mechatronics/Activation_functions.jpg" alt="" width="600" height="130">
<br>
이 그림에서 w는 가중치이고 b는 bias이다.

딥러닝의 기본 개념에서 구분에 대해서 공부할 수 있는데 이 구분의 요소에서 XOR의 문제가 나타나게 된다.

이후에 XOR에 대해서 Linear하게 구분할 수 없음을 증명하는 것이 나오게 되었다.<br>
그 후에 Backpropagation이라는 방법이 나오는데 이 알고리즘은 error를 label쪽에서 정해 w(가중치),b(bias)를 조절하는 방법이다.
이로인해 XOR과 복잡한 형식의 예측이 가능해지게 되었다.<br>

그리고 Convolutional Neural Networks라는 개념이 나왔는데 어떠한 특정 그림을 보았을 때 일부의 뉴런의 신호가 증폭되는것에 착안해 생겨진 방법이다. 부분부분을 잘라서 학습하고 이후에 더하는 방식이다.
<img src="/img/mechatronics/convolution_1.jpg" alt="" width="600" height="130">
<br>

그런데 Backpropagation에 문제가 생기게 됬는데 많은 layer가 있을수록 error의 의미가 뒤로갈수록 낮아지게 되서 많은 backpropagation일수록 성능이 떨어지게 되었다.